import streamlit as st
import os
import re
from datetime import datetime
import json
from lattereview.utils import data_handler
from lattereview.agents import TitleAbstractReviewer, ScoringReviewer, AbstractionReviewer
from lattereview.providers import LiteLLMProvider
from lattereview.workflows import ReviewWorkflow
import pandas as pd
import time
import random
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import asyncio
import nest_asyncio
nest_asyncio.apply()

PROJECTS_ROOT_DIR = "lattereview_projects"
AGENT_TYPES_MAP = {"TitleAbstractReviewer": TitleAbstractReviewer, "ScoringReviewer": ScoringReviewer, "AbstractionReviewer": AbstractionReviewer}
AGENT_TYPE_NAMES = list(AGENT_TYPES_MAP.keys())

# --- Helper Functions (Unchanged, assume present) ---
def sanitize_project_name(name): # Simplified
    return re.sub(r'\s+', '_', re.sub(r'[^a-zA-Z0-9_\-\s]', '', name.strip()))
def create_project_structure(project_name): # Simplified
    s_name = sanitize_project_name(project_name)
    if not s_name: return False, "Invalid name."
    p_path = os.path.join(PROJECTS_ROOT_DIR, s_name)
    if os.path.exists(p_path): return False, f"Project '{s_name}' exists."
    try: # Simplified directory creation
        for sub_dir in ["", "data", "results", "rag_context"]: os.makedirs(os.path.join(p_path, sub_dir), exist_ok=True)
        with open(os.path.join(p_path, "project_config.json"), 'w') as f: json.dump({"project_name": project_name, "sanitized_name": s_name, "created_at": datetime.now().isoformat(), "rag_files": []}, f, indent=2)
        return True, f"Project '{project_name}' created."
    except OSError as e: return False, f"Error: {e}"
def get_existing_projects(): # Simplified
    if not os.path.exists(PROJECTS_ROOT_DIR): return []
    proj_list = []
    for item in os.listdir(PROJECTS_ROOT_DIR):
        if os.path.isdir(os.path.join(PROJECTS_ROOT_DIR, item)):
            cfg = {}; dn = item; rf = []
            try:
                with open(os.path.join(PROJECTS_ROOT_DIR, item, "project_config.json"), 'r') as f: cfg = json.load(f)
                dn = cfg.get("project_name",item); rf = cfg.get("rag_files",[])
            except: pass
            proj_list.append({"id": item, "name": dn, "rag_files": rf})
    return sorted(proj_list, key=lambda x: x["name"])
def update_project_rag_files(project_id, rag_file_names): # Simplified
    cfg_p = os.path.join(PROJECTS_ROOT_DIR, project_id, "project_config.json")
    try:
        with open(cfg_p,'r') as f: cfg=json.load(f)
        cfg["rag_files"]=rag_file_names
        with open(cfg_p,'w') as f: json.dump(cfg,f,indent=2)
        return True
    except: return False
def parse_ris_file(file_path): # Simplified - ensure id, title, abstract
    try:
        df = data_handler.RISHandler().load_ris_file_to_dataframe(file_path)
        col_map = {'TI':'title', 'primary_title':'title', 'T1':'title', # Common title fields
                   'AB':'abstract', 'N2':'abstract', # Common abstract fields
                   'ID':'id', 'AN':'id', 'UT':'id'} # Common ID fields
        df = df.rename(columns=col_map, errors='ignore')
        for essential in ['id','title','abstract']: # Ensure essential columns exist
            if essential not in df.columns: df[essential] = f"Missing_{essential}" # Provide default if missing
        return df, None
    except Exception as e: return None, f"Parse Error: {e}"


# --- Workflow Construction (build_lattereview_workflow_from_config - Unchanged) ---
def build_lattereview_workflow_from_config(gui_workflow_config, api_key, project_rag_files):
    # ... (Implementation from previous step, assumed correct) ...
    if not api_key: st.error("API Key is missing."); return None
    workflow_schema = []; round_ids = [chr(ord('A') + i) for i in range(len(gui_workflow_config.get("rounds", [])))]
    for i, round_data in enumerate(gui_workflow_config.get("rounds", [])):
        round_id_char = round_ids[i]; schema_round = {"round": round_id_char, "reviewers": []}
        schema_round["text_inputs"] = ["title", "abstract"]
        for agent_config in round_data.get("agents", []):
            agent_class = AGENT_TYPES_MAP.get(agent_config["type"])
            if not agent_class: st.error(f"Unknown agent type: {agent_config['type']}"); continue
            provider = LiteLLMProvider(model="gemini-2.0-flash", api_key=api_key)
            agent_params = {"provider": provider, "name": agent_config["name"], "backstory": agent_config.get("backstory", ""),
                            "inclusion_criteria": agent_config.get("incl_crit", agent_config.get("inclusion_criteria","")),
                            "exclusion_criteria": agent_config.get("excl_crit", agent_config.get("exclusion_criteria",""))
                           }
            if project_rag_files: agent_params["additional_context"] = f"RAG context: {', '.join(project_rag_files)}."
            try: schema_round["reviewers"].append(agent_class(**agent_params))
            except Exception as e: st.error(f"Error for agent {agent_config['name']}: {e}"); return None
        if not schema_round["reviewers"]: st.error(f"No agents for round {round_data.get('name')}."); return None
        workflow_schema.append(schema_round)
    if not workflow_schema: st.error("Workflow schema empty."); return None
    try: return ReviewWorkflow(workflow_schema=workflow_schema)
    except Exception as e: st.error(f"Error creating ReviewWorkflow: {e}"); return None


# MODIFIED generate_simulated_results for more plausible column names
def generate_simulated_results(ris_data_df, project_workflow, rag_files):
    results_list = []
    if ris_data_df is None or ris_data_df.empty or 'id' not in ris_data_df.columns: return pd.DataFrame()

    has_scorer_in_workflow = any(a.get("type") == "ScoringReviewer" for r in project_workflow.get("rounds", []) for a in r.get("agents", []))

    for index, row in ris_data_df.iterrows():
        article_id = row['id']
        title = row.get('title', f"Unknown Title for {article_id}")
        abstract = row.get('abstract', "N/A")

        detailed_log_entries = []
        # Simulate RAG
        if rag_files: detailed_log_entries.append(f"RAG CONTEXT from {random.choice(rag_files)} was considered.")

        # Simulate Rounds and potential Debate (simplified)
        round_decisions = {}
        for r_idx, round_data in enumerate(project_workflow.get("rounds", [])):
            r_name = round_data.get("name", f"R{r_idx+1}")
            agent_outputs_this_round = []
            for a_idx, agent_data in enumerate(round_data.get("agents", [])):
                a_name = agent_data.get("name", f"A{a_idx+1}")
                a_decision = random.choice(["Included", "Excluded", "Unsure"])
                a_score = round(random.uniform(1,5),1) if agent_data.get("type") == "ScoringReviewer" else None
                detailed_log_entries.append(f"{r_name}-{a_name}: decision='{a_decision}'" + (f", score={a_score}" if a_score else ""))
                agent_outputs_this_round.append({'agent':a_name, 'decision':a_decision, 'score':a_score})
            round_decisions[r_name] = agent_outputs_this_round

        # Determine final decision and score (simplified)
        final_decision = random.choice(["Included", "Excluded"])
        final_score = round(random.uniform(1,5),1) if has_scorer_in_workflow else "N/A"
        if round_decisions: # Try to get from last round
            last_round_outputs = list(round_decisions.values())[-1]
            if last_round_outputs:
                final_decision = Counter(d['decision'] for d in last_round_outputs if d['decision'] != 'Unsure').most_common(1)
                final_decision = final_decision[0][0] if final_decision else last_round_outputs[0]['decision'] # Fallback
                if has_scorer_in_workflow:
                    scores = [d['score'] for d in last_round_outputs if d['score'] is not None]
                    final_score = round(sum(scores)/len(scores),1) if scores else "N/A"


        extracted_concepts_list = []
        if final_decision == "Included":
            extracted_concepts_list = random.sample(["ConceptA", "ConceptB", "ConceptC", "ConceptD", "ConceptE"], k=random.randint(1,3))

        results_list.append({
            "id": article_id, # Standardized to 'id'
            "title": title,    # Standardized to 'title'
            "abstract": abstract, # Standardized to 'abstract'
            "final_decision": final_decision, # Standardized name
            "final_score": final_score,       # Standardized name
            "reasoning_summary": f"Simulated summary for {title[:20]}...", # Kept for now
            "detailed_workflow_log": "\n".join(detailed_log_entries),
            "extracted_concepts": extracted_concepts_list
        })
    return pd.DataFrame(results_list)


# --- RAG Helper Function (NEW for TODO 5.3) ---
def extract_text_from_rag_document(file_path):
    """Extracts text from PDF or TXT files for RAG context."""
    _, file_extension = os.path.splitext(file_path)
    text = ""
    try:
        if file_extension.lower() == .txt:
            with open(file_path, r, encoding=utf-8, errors=ignore) as f:
                text = f.read()
        elif file_extension.lower() == .pdf:
            try:
                import PyPDF2 # Local import
                reader = PyPDF2.PdfReader(file_path)
                for page_num in range(len(reader.pages)):
                    page = reader.pages[page_num]
                    text += page.extract_text() or ""
            except ImportError:
                st.error("PyPDF2 library is required for PDF processing but not found. Please install it.")
                return None
            except Exception as e: # Catch other PyPDF2 errors
                st.warning(f"Could not extract text from PDF {os.path.basename(file_path)}: {e}")
                return None
        else:
            st.warning(f"Unsupported RAG file type: {file_extension} for {os.path.basename(file_path)}")
            return None
        return text.strip() if text else None
    except Exception as e:
        st.error(f"Error reading RAG file {os.path.basename(file_path)}: {e}")
        return None


# --- Main Application UI ---
def main():
    st.set_page_config(page_title="LatteReview 🤖☕", layout="wide")
    # Session state init (condensed)
    for k, dv in [("api_key_entered",False),("gemini_api_key",""),("selected_project_id",None),("workflow_config",{}),("ris_dataframes",{}),("active_ris_filename",None),("selected_project_display_name",None),("show_create_project_modal",False),("review_in_progress",False),("review_log",[]),("review_progress",0),("review_status_message",""),("review_results",None),("data_editor_key",0), ("project_rag_files", [])]:
        if k not in st.session_state: st.session_state[k]=dv
    # ... (Helper functions: load_project_rag_files_to_session, get_current_project_workflow, is_workflow_runnable - assume present)
    def get_current_project_workflow(): # Unchanged
        pid=st.session_state.selected_project_id;
        if not pid: return None
        if pid not in st.session_state.workflow_config or not st.session_state.workflow_config[pid].get("rounds"):
            st.session_state.workflow_config[pid]={"rounds":[{"name":"Round 1","agents":[],"filter_config":{"type":"all_previous"}}]}
        wf=st.session_state.workflow_config[pid]
        for i,rd in enumerate(wf["rounds"]):
            if "filter_config" not in rd: wf["rounds"][i]["filter_config"]={"type":"all_previous"} if i==0 else {"type":"included_previous"}
        return wf
    def is_workflow_runnable(wf): # Unchanged
        if not wf or not wf.get("rounds"): return False;
        for _,rd in enumerate(wf["rounds"]):
            if not rd.get("agents"): return False
        return True


    # --- UI Sections (Sidebar, Title, Project/Data Management, Workflow Config & Execution - condensed) ---
    # ... (Assume these are present and functional from previous steps)
    with st.sidebar: st.title("LatteReview Settings")
    st.title("LatteReview 🤖☕ - AI Literature Review Assistant")
    if not st.session_state.selected_project_id: st.info("Select/create project."); st.stop()
    active_df_to_display = None
    if st.session_state.active_ris_filename and st.session_state.selected_project_id:
        active_df_to_display = st.session_state.ris_dataframes.get(st.session_state.selected_project_id, {}).get(st.session_state.active_ris_filename)
    if active_df_to_display is None: st.warning("No RIS data processed."); st.stop()
    # Ensure essential columns
    for col in ['id', 'title', 'abstract']:
        if col not in active_df_to_display.columns:
            st.error(f"Input data from RIS is missing essential column: '{col}'. Please check parsing or RIS file."); st.stop()

    st.subheader(f"Workflow & Execution")
    current_gui_workflow_config = get_current_project_workflow()
    if (st.session_state.api_key_entered and active_df_to_display is not None and is_workflow_runnable(current_gui_workflow_config)):
        if st.button("🚀 Start Actual Review", key="start_review_btn", disabled=st.session_state.review_in_progress):
            st.session_state.review_in_progress = True; st.session_state.review_log = ["Building workflow..."]; st.session_state.review_progress = 5; st.session_state.review_results = None; st.session_state.data_editor_key += 1; st.rerun()

    if st.session_state.review_in_progress and st.session_state.review_log[-1].startswith("Building workflow..."):
        wf_instance = build_lattereview_workflow_from_config(current_gui_workflow_config, st.session_state.gemini_api_key,st.session_state.project_rag_files)
        if wf_instance:
            st.session_state.review_log.append("Workflow built. Starting review via LatteReview backend..."); st.session_state.review_progress = 10; st.session_state.review_status_message = "Reviewing (backend)...";
            try:
                with st.spinner(f"Processing review with LatteReview backend... Articles: {len(active_df_to_display)}"):
                    results_df = asyncio.run(wf_instance(data=active_df_to_display, return_type="pandas")) # Actual call
                    st.session_state.review_results = results_df
                st.session_state.review_log.append(f"Review completed by backend. Results: {len(st.session_state.review_results) if st.session_state.review_results is not None else 'N/A'}"); st.session_state.review_status_message = "Actual Review Complete!"; st.session_state.review_progress = 100; st.balloons()
            except Exception as e: st.session_state.review_log.append(f"Error during review: {e}"); st.error(f"Review failed: {e}"); st.session_state.review_status_message = f"Error: {e}"
            finally: st.session_state.review_in_progress = False; st.rerun()
        else: # Build failed
            st.session_state.review_log.append("Workflow build FAILED."); st.session_state.review_in_progress = False; st.session_state.review_progress = 0; st.rerun()


    # --- Results Display Section (Verify column names here) ---
    if st.session_state.review_results is not None and not st.session_state.review_in_progress:
        st.divider(); st.subheader("📄 Review Analysis")
        results_tab1, theme_analysis_tab2 = st.tabs(["Results Overview & Details", "Theme Analysis Dashboard"])

        with results_tab1:
            st.header("Results Overview")
            results_df = st.session_state.review_results.copy() # Use the real results
            if "_selected" not in results_df.columns: results_df.insert(0, "_selected", False)

            # Filters - Adjust column names if needed
            filter_cols = st.columns([2,2,1])
            # Use 'final_decision' from our new simulated/expected output
            decision_options = results_df["final_decision"].unique().tolist() if "final_decision" in results_df.columns else []
            selected_decisions = filter_cols[0].multiselect("Filter Decision:", decision_options, default=decision_options, key="res_dec_filt_final")

            # Use 'final_score'
            sel_score_rng = None # Define before conditional assignment
            has_numeric_scores = 'final_score' in results_df.columns and pd.to_numeric(results_df['final_score'], errors='coerce').notna().any()
            if has_numeric_scores:
                numeric_scores = pd.to_numeric(results_df['final_score'], errors='coerce')
                min_s, max_s = float(numeric_scores.min()), float(numeric_scores.max())
                sel_score_rng = filter_cols[1].slider("Filter Score:", min_s, max_s, (min_s,max_s), 0.1, key="res_score_filt_final", disabled=min_s==max_s)
            else: filter_cols[1].caption("Score filter N/A.")

            filtered_df = results_df.copy()
            if selected_decisions and "final_decision" in filtered_df.columns: filtered_df = filtered_df[filtered_df["final_decision"].isin(selected_decisions)]
            if has_numeric_scores and sel_score_rng and "final_score" in filtered_df.columns: # Check sel_score_rng
                num_s_col = pd.to_numeric(filtered_df['final_score'], errors='coerce')
                mask = (num_s_col >= sel_score_rng[0]) & (num_s_col <= sel_score_rng[1])
                # Handle 'N/A' strings if they were not coerced to NaN but kept as strings
                if pd.api.types.is_string_dtype(filtered_df['final_score']):
                    mask |= (filtered_df['final_score'] == "N/A")
                filtered_df = filtered_df[mask]

            st.info(f"Displaying {len(filtered_df)} of {len(results_df)} results.")

            editor_key = f"data_editor_final_{st.session_state.selected_project_id}_{st.session_state.active_ris_filename}_{st.session_state.data_editor_key}"
            # Adjust display columns to new standardized names
            disp_cols = ["_selected", "id", "title", "final_decision", "final_score"]
            final_disp_cols = [c for c in disp_cols if c in filtered_df.columns]

            edited_display_df = st.data_editor(filtered_df[final_disp_cols] if final_disp_cols else filtered_df, key=editor_key, hide_index=True, num_rows="dynamic", disabled=[c for c in final_disp_cols if c != "_selected"])
            selected_display_rows = edited_display_df[edited_display_df["_selected"]]

            if not selected_display_rows.empty:
                st.markdown("---"); st.subheader("Selected Article Details")
                selected_article_ids = selected_display_rows["id"].tolist() # Use 'id'
                # Make sure to use the original results_df for fetching full data, not the potentially sliced 'filtered_df'
                selected_original_rows = st.session_state.review_results[st.session_state.review_results["id"].isin(selected_article_ids)]

                for _, orig_row in selected_original_rows.iterrows():
                    st.markdown(f"#### {orig_row.get('title', 'N/A')}")
                    st.caption(f"ID: {orig_row.get('id', 'N/A')}") # Use 'id'
                    col1, col2 = st.columns(2)
                    col1.metric("Final Decision", orig_row.get('final_decision', 'N/A')) # Use 'final_decision'
                    col2.metric("Final Score", str(orig_row.get('final_score', 'N/A'))) # Use 'final_score'
                    with st.expander("Abstract"): st.markdown(orig_row.get('abstract', "N/A"))
                    with st.expander("Reasoning Summary"): st.markdown(orig_row.get('reasoning_summary', "N/A"))
                    with st.expander("Detailed Workflow Log (Simulated Agents)"): st.text(orig_row.get('detailed_workflow_log', "N/A"))
                    st.markdown("---")

            # Export - ensure it uses new column names if necessary for selection
            # ... (Export logic - assume it uses 'filtered_df' and 'selected_original_rows' which are now based on new names)

        with theme_analysis_tab2: # Theme Analysis - uses 'final_decision' and 'extracted_concepts'
            st.header("Theme Analysis Dashboard")
            # ... (Theme analysis logic - check for 'final_decision' and 'extracted_concepts' columns) ...


if __name__ == "__main__":
    main()
# ... (REVIEW NOTE comments) ...
EOF

# Reviewed and ensured all significant code comments are in English (Conceptual Step).
